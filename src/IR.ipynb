{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kflXyNg6SsEl"
      },
      "source": [
        "imports and connect to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNbmhYASSk5f",
        "outputId": "45448efb-277f-4964-dbd7-3f5843bb8113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.8/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "import string\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Tensor flow\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Langdetect \n",
        "!pip install langdetect\n",
        "import langdetect\n",
        "\n",
        "# Connect drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYHIaZtEEOcY"
      },
      "source": [
        "Initialize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3HrdZxOScfg",
        "outputId": "8b22cafa-b734-42b6-934f-00f47b400489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (2,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full data set contains 441936 comments belonging to 2265 videos.\n",
            "             content  label\n",
            "category_id                \n",
            "1               9159   9159\n",
            "10             21488  21488\n",
            "17              6862   6862\n",
            "22             21282  21282\n",
            "23             19972  19972\n",
            "24             37945  37945\n",
            "25             13565  13565\n",
            "26             23854  23854\n",
            "27              8845   8845\n",
            "28             13399  13399\n",
            "Filtered data set contains 176371 comments belonging to 2122 videos.\n"
          ]
        }
      ],
      "source": [
        "categories = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/categories.csv\")\n",
        "videos = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/videos.csv\", on_bad_lines='skip')[[\"id\", \"category_id\"]].drop_duplicates().set_index('id')\n",
        "comments = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/comments.csv\", on_bad_lines='skip')[[\"video_id\", \"content\"]].drop_duplicates().set_index('video_id')\n",
        "rem = set(comments.index) - set(videos.index)\n",
        "for r in rem: comments.drop(r)\n",
        "df = comments.join(videos).drop_duplicates()\n",
        "df = df[[\"category_id\", \"content\"]]\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Preprocess\n",
        "def remove_emojis(data):\n",
        "    try:\n",
        "        emoj = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            u\"\\U0001f926-\\U0001f937\"\n",
        "            u\"\\U00010000-\\U0010ffff\"\n",
        "            u\"\\u2640-\\u2642\" \n",
        "            u\"\\u2600-\\u2B55\"\n",
        "            u\"\\u200d\"\n",
        "            u\"\\u23cf\"\n",
        "            u\"\\u23e9\"\n",
        "            u\"\\u231a\"\n",
        "            u\"\\ufe0f\"  # dingbats\n",
        "            u\"\\u3030\"\n",
        "                          \"]+\", re.UNICODE)\n",
        "        return re.sub(emoj, '', data)\n",
        "    except:\n",
        "        return data\n",
        "\n",
        "def llower(data):\n",
        "  try:\n",
        "      return data.lower()\n",
        "  except:\n",
        "    return data\n",
        "\n",
        "def ttranslate(data):\n",
        "  try:\n",
        "      return data.translate(str.maketrans('', '', string.punctuation))\n",
        "  except:\n",
        "    return data\n",
        "\n",
        "\n",
        "df[\"content\"] = df[\"content\"].apply(lambda row: remove_emojis(row))\n",
        "df[\"content\"] = df[\"content\"].apply(lambda row: llower(row))\n",
        "df[\"content\"] = df[\"content\"].apply(lambda row: ttranslate(row))\n",
        "\n",
        "\n",
        "print(\"Full data set contains {} comments belonging to {} videos.\".format(len(df), len(df.index.unique())))\n",
        "category = {\n",
        "    1: 0,\n",
        "    #2: 1,\n",
        "    10: 1,\n",
        "    #15: 3,\n",
        "    17: 2,\n",
        "    #18: 2,\n",
        "    #19: 6,\n",
        "    #20: 7,\n",
        "    #21: 3,\n",
        "    22: 3,\n",
        "    23: 4,\n",
        "    24: 5,\n",
        "    25: 6,\n",
        "    26: 7,\n",
        "    27: 8,\n",
        "    28: 9,\n",
        "    #29: 16\n",
        "}\n",
        "\n",
        "def is_en(content):\n",
        "    try:\n",
        "        if not (50 < len(content) < 600): return False\n",
        "        return langdetect.detect(content[:100]) == \"en\"\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def label(category_id):\n",
        "    label = [0] * len(category.keys())\n",
        "    label[category[category_id]] = 1\n",
        "    return label\n",
        "\n",
        "df = df[df['category_id'].isin(category.keys())]\n",
        "df = df[df['content'].apply(lambda content: is_en(content))]\n",
        "df[\"label\"] = df.apply(lambda row: label(row[\"category_id\"]), axis=1)\n",
        "print(df.groupby(\"category_id\").count())\n",
        "print(\"Filtered data set contains {} comments belonging to {} videos.\".format(len(df), len(df.index.unique())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pk8wLMdpqtj"
      },
      "source": [
        "Divide data set in train, validate and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Foy7Y3v_ptyr",
        "outputId": "3a952732-b5a0-4593-8082-ad9ec4999062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data set contains 141096 comments\n",
            "Test data set contains 10000 comments\n",
            "Validation data set contains 25275 comments\n"
          ]
        }
      ],
      "source": [
        "train, validate, test = np.array_split(df, [int(0.8*len(df)), max(len(df)-10000, int(0.9*len(df)))])\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train['content'].to_numpy(), list(train['label'].values)))\n",
        "print(\"Train data set contains {} comments\".format(len(train_data)))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test['content'].to_numpy(), list(test['label'].values)))\n",
        "print(\"Test data set contains {} comments\".format(len(test_data)))\n",
        "validation_data = tf.data.Dataset.from_tensor_slices((validate['content'].to_numpy(), list(validate['label'].values)))\n",
        "print(\"Validation data set contains {} comments\".format(len(validation_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts7Pc4lWEWtg"
      },
      "source": [
        "Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5LWOt7htVHh",
        "outputId": "d5e160d0-59dc-49a3-ff63-a5c8dfaf948a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"YTBCommentClassifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 512)               256797824 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              2101248   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2048)              8390656   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              2098176   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 269,398,154\n",
            "Trainable params: 12,600,330\n",
            "Non-trainable params: 256,797,824\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the encoder\n",
        "encoder = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "    input_shape=[], \n",
        "    dtype=tf.string, \n",
        "    trainable=False\n",
        ")\n",
        "\n",
        "# Create the classifier (DNN)\n",
        "classifier = tf.keras.Sequential(\n",
        "    [\n",
        "        encoder,\n",
        "        tf.keras.layers.Dense(4096, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(2048, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1024, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(len(category.keys()), activation=\"softmax\")\n",
        "    ],\n",
        "    name = \"YTBCommentClassifier\"\n",
        ")\n",
        "\n",
        "# Summarize and compile the model\n",
        "classifier.summary()\n",
        "classifier.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dzZ962LF0sS"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "E5wQkFSOF17s",
        "outputId": "e25180c8-7d7c-4c28-a6e4-16b422780c5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "138/138 [==============================] - 29s 170ms/step - loss: 1.6690 - accuracy: 0.4046 - val_loss: 1.5255 - val_accuracy: 0.4607\n",
            "Epoch 2/10\n",
            "138/138 [==============================] - 24s 171ms/step - loss: 1.4838 - accuracy: 0.4766 - val_loss: 1.4413 - val_accuracy: 0.4940\n",
            "Epoch 3/10\n",
            "138/138 [==============================] - 23s 169ms/step - loss: 1.3752 - accuracy: 0.5200 - val_loss: 1.3988 - val_accuracy: 0.5096\n",
            "Epoch 4/10\n",
            "138/138 [==============================] - 23s 167ms/step - loss: 1.2802 - accuracy: 0.5528 - val_loss: 1.3885 - val_accuracy: 0.5198\n",
            "Epoch 5/10\n",
            "138/138 [==============================] - 23s 167ms/step - loss: 1.1921 - accuracy: 0.5844 - val_loss: 1.3797 - val_accuracy: 0.5236\n",
            "Epoch 6/10\n",
            "138/138 [==============================] - 23s 168ms/step - loss: 1.1143 - accuracy: 0.6115 - val_loss: 1.3925 - val_accuracy: 0.5238\n",
            "Epoch 7/10\n",
            "138/138 [==============================] - 24s 177ms/step - loss: 1.0411 - accuracy: 0.6359 - val_loss: 1.4197 - val_accuracy: 0.5207\n",
            "Epoch 8/10\n",
            " 86/138 [=================>............] - ETA: 7s - loss: 0.9813 - accuracy: 0.6566"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e884c9171a6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "batch_size = 1024\n",
        "epochs = 10\n",
        "history = classifier.fit(train_data.batch(batch_size), epochs=epochs, validation_data=validation_data.batch(batch_size), verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOGOkLxhwF0"
      },
      "source": [
        "Calculate all embeddings for data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGvDKIGPhwaI"
      },
      "outputs": [],
      "source": [
        "# Calculate embeddings of train & validate data\n",
        "data = pd.concat([train, validate])\n",
        "data[\"embedding\"] = data[\"content\"].apply(lambda content: encoder([content])[0])\n",
        "data[\"video_id\"] = data.index\n",
        "data = data.groupby(level=0, as_index=False).agg({'video_id': 'first', 'category_id': 'first', 'embedding': lambda x: list(x)})\n",
        "data[\"embedding\"] = data[\"embedding\"].apply(lambda row: tf.convert_to_tensor(row))\n",
        "data = data.set_index(\"video_id\")\n",
        "data[\"embedding\"] = data[\"embedding\"].apply(lambda row: tf.math.reduce_mean(row, axis=0, keepdims=False, name=None))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16gEjcghUQQ-"
      },
      "source": [
        "Evaluate model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FwSVFLYnUTFR",
        "outputId": "cfa92bc0-7384-4f1c-9c9a-25bfea977698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier accuracy on test set was: 0.58\n",
            "Hitrate of our first recommendation was: [0.34, 0.4, 0.42, 0.44, 0.44, 0.46]\n",
            "Hitrate of our second recommendation was: [0.41, 0.5, 0.53, 0.56, 0.56, 0.61]\n",
            "Hitrate of our combined recommendation was: [0.39, 0.49, 0.52, 0.55, 0.57, 0.61]\n",
            "0.01 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.55\n",
            "Hitrate of our first recommendation was: [0.33, 0.385, 0.4, 0.42, 0.42, 0.46]\n",
            "Hitrate of our second recommendation was: [0.38, 0.465, 0.485, 0.52, 0.525, 0.57]\n",
            "Hitrate of our combined recommendation was: [0.375, 0.455, 0.49, 0.515, 0.525, 0.58]\n",
            "0.02 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5333333333333333\n",
            "Hitrate of our first recommendation was: [0.3, 0.3467, 0.3667, 0.3833, 0.3833, 0.42]\n",
            "Hitrate of our second recommendation was: [0.3433, 0.4267, 0.4467, 0.4767, 0.4833, 0.5333]\n",
            "Hitrate of our combined recommendation was: [0.3333, 0.41, 0.4467, 0.47, 0.49, 0.5433]\n",
            "0.03 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.56\n",
            "Hitrate of our first recommendation was: [0.34, 0.39, 0.4125, 0.4275, 0.4275, 0.4575]\n",
            "Hitrate of our second recommendation was: [0.3875, 0.4675, 0.4975, 0.5275, 0.5325, 0.5725]\n",
            "Hitrate of our combined recommendation was: [0.375, 0.4525, 0.4975, 0.515, 0.5375, 0.5825]\n",
            "0.04 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.558\n",
            "Hitrate of our first recommendation was: [0.332, 0.394, 0.412, 0.426, 0.43, 0.46]\n",
            "Hitrate of our second recommendation was: [0.38, 0.464, 0.494, 0.522, 0.538, 0.584]\n",
            "Hitrate of our combined recommendation was: [0.374, 0.456, 0.506, 0.524, 0.544, 0.588]\n",
            "0.05 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5666666666666667\n",
            "Hitrate of our first recommendation was: [0.3367, 0.4, 0.4167, 0.4317, 0.4367, 0.47]\n",
            "Hitrate of our second recommendation was: [0.375, 0.465, 0.4967, 0.52, 0.5383, 0.59]\n",
            "Hitrate of our combined recommendation was: [0.3717, 0.4583, 0.505, 0.5233, 0.5417, 0.59]\n",
            "0.06 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5714285714285714\n",
            "Hitrate of our first recommendation was: [0.3429, 0.4029, 0.4214, 0.4357, 0.44, 0.4757]\n",
            "Hitrate of our second recommendation was: [0.3771, 0.4643, 0.4943, 0.52, 0.5414, 0.5957]\n",
            "Hitrate of our combined recommendation was: [0.3743, 0.4586, 0.5043, 0.5243, 0.5443, 0.5929]\n",
            "0.07 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.57375\n",
            "Hitrate of our first recommendation was: [0.345, 0.4012, 0.4238, 0.4375, 0.4437, 0.48]\n",
            "Hitrate of our second recommendation was: [0.375, 0.4587, 0.4925, 0.5175, 0.5413, 0.5988]\n",
            "Hitrate of our combined recommendation was: [0.3738, 0.4562, 0.5, 0.5238, 0.5437, 0.5975]\n",
            "0.08 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5666666666666667\n",
            "Hitrate of our first recommendation was: [0.3344, 0.3856, 0.4089, 0.4244, 0.4311, 0.4678]\n",
            "Hitrate of our second recommendation was: [0.3678, 0.4489, 0.4822, 0.5078, 0.5322, 0.59]\n",
            "Hitrate of our combined recommendation was: [0.3667, 0.4456, 0.4867, 0.51, 0.5289, 0.59]\n",
            "0.09 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.57\n",
            "Hitrate of our first recommendation was: [0.335, 0.386, 0.41, 0.427, 0.435, 0.471]\n",
            "Hitrate of our second recommendation was: [0.366, 0.445, 0.483, 0.511, 0.536, 0.595]\n",
            "Hitrate of our combined recommendation was: [0.366, 0.446, 0.488, 0.514, 0.533, 0.596]\n",
            "0.1 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5618181818181818\n",
            "Hitrate of our first recommendation was: [0.3236, 0.3782, 0.4018, 0.4191, 0.4264, 0.4609]\n",
            "Hitrate of our second recommendation was: [0.3545, 0.4318, 0.4736, 0.5027, 0.5264, 0.59]\n",
            "Hitrate of our combined recommendation was: [0.3536, 0.4327, 0.4764, 0.5027, 0.5245, 0.5891]\n",
            "0.11 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5566666666666666\n",
            "Hitrate of our first recommendation was: [0.3167, 0.3692, 0.3933, 0.4125, 0.42, 0.4558]\n",
            "Hitrate of our second recommendation was: [0.3483, 0.4258, 0.4675, 0.4975, 0.5208, 0.5858]\n",
            "Hitrate of our combined recommendation was: [0.3475, 0.4267, 0.4692, 0.495, 0.5175, 0.5833]\n",
            "0.12 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.553076923076923\n",
            "Hitrate of our first recommendation was: [0.3131, 0.3638, 0.3885, 0.4085, 0.4162, 0.4515]\n",
            "Hitrate of our second recommendation was: [0.3438, 0.4185, 0.46, 0.4892, 0.5131, 0.5785]\n",
            "Hitrate of our combined recommendation was: [0.3438, 0.4208, 0.4623, 0.4892, 0.5108, 0.5769]\n",
            "0.13 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5464285714285714\n",
            "Hitrate of our first recommendation was: [0.3064, 0.3593, 0.385, 0.405, 0.4121, 0.445]\n",
            "Hitrate of our second recommendation was: [0.34, 0.4136, 0.4571, 0.4864, 0.5129, 0.5779]\n",
            "Hitrate of our combined recommendation was: [0.34, 0.4157, 0.4586, 0.4886, 0.51, 0.5757]\n",
            "0.14 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5473333333333333\n",
            "Hitrate of our first recommendation was: [0.306, 0.3607, 0.3853, 0.404, 0.4113, 0.444]\n",
            "Hitrate of our second recommendation was: [0.336, 0.4127, 0.4553, 0.486, 0.512, 0.576]\n",
            "Hitrate of our combined recommendation was: [0.3373, 0.4133, 0.456, 0.486, 0.5073, 0.5753]\n",
            "0.15 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5525\n",
            "Hitrate of our first recommendation was: [0.31, 0.3631, 0.3887, 0.4075, 0.4156, 0.45]\n",
            "Hitrate of our second recommendation was: [0.3406, 0.415, 0.4569, 0.4875, 0.5119, 0.5787]\n",
            "Hitrate of our combined recommendation was: [0.3412, 0.4144, 0.4587, 0.4881, 0.5094, 0.58]\n",
            "0.16 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5529411764705883\n",
            "Hitrate of our first recommendation was: [0.3094, 0.3635, 0.39, 0.4088, 0.4165, 0.4506]\n",
            "Hitrate of our second recommendation was: [0.3435, 0.4165, 0.4576, 0.49, 0.5135, 0.58]\n",
            "Hitrate of our combined recommendation was: [0.3441, 0.4159, 0.4618, 0.4912, 0.5124, 0.5812]\n",
            "0.17 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5522222222222222\n",
            "Hitrate of our first recommendation was: [0.3083, 0.3622, 0.3883, 0.4072, 0.415, 0.4511]\n",
            "Hitrate of our second recommendation was: [0.3433, 0.4161, 0.4567, 0.49, 0.5128, 0.5806]\n",
            "Hitrate of our combined recommendation was: [0.3444, 0.415, 0.4611, 0.4906, 0.5128, 0.5828]\n",
            "0.18 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5563157894736842\n",
            "Hitrate of our first recommendation was: [0.3058, 0.3616, 0.3884, 0.4074, 0.4153, 0.4521]\n",
            "Hitrate of our second recommendation was: [0.34, 0.4126, 0.4542, 0.4879, 0.5105, 0.5784]\n",
            "Hitrate of our combined recommendation was: [0.3395, 0.4126, 0.4589, 0.4895, 0.5116, 0.5811]\n",
            "0.19 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.555\n",
            "Hitrate of our first recommendation was: [0.3055, 0.36, 0.3865, 0.4055, 0.414, 0.4515]\n",
            "Hitrate of our second recommendation was: [0.343, 0.414, 0.455, 0.488, 0.5105, 0.58]\n",
            "Hitrate of our combined recommendation was: [0.34, 0.4135, 0.459, 0.4895, 0.513, 0.5815]\n",
            "0.2 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5533333333333333\n",
            "Hitrate of our first recommendation was: [0.3067, 0.36, 0.3867, 0.4048, 0.4133, 0.4505]\n",
            "Hitrate of our second recommendation was: [0.3457, 0.4171, 0.4581, 0.491, 0.5124, 0.58]\n",
            "Hitrate of our combined recommendation was: [0.3433, 0.4171, 0.4619, 0.4924, 0.5148, 0.5814]\n",
            "0.21 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5518181818181818\n",
            "Hitrate of our first recommendation was: [0.3036, 0.3564, 0.3827, 0.4005, 0.4095, 0.4468]\n",
            "Hitrate of our second recommendation was: [0.3459, 0.415, 0.4559, 0.4886, 0.5114, 0.5814]\n",
            "Hitrate of our combined recommendation was: [0.3427, 0.4159, 0.4605, 0.49, 0.5123, 0.5823]\n",
            "0.22 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5482608695652174\n",
            "Hitrate of our first recommendation was: [0.2996, 0.3535, 0.3796, 0.3983, 0.407, 0.4443]\n",
            "Hitrate of our second recommendation was: [0.3422, 0.4126, 0.453, 0.4865, 0.5087, 0.58]\n",
            "Hitrate of our combined recommendation was: [0.3387, 0.4135, 0.4578, 0.4878, 0.5096, 0.5813]\n",
            "0.23 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5466666666666666\n",
            "Hitrate of our first recommendation was: [0.3008, 0.3542, 0.38, 0.3983, 0.4067, 0.4437]\n",
            "Hitrate of our second recommendation was: [0.3442, 0.4142, 0.4554, 0.4892, 0.5108, 0.5833]\n",
            "Hitrate of our combined recommendation was: [0.3404, 0.4142, 0.4587, 0.4888, 0.5112, 0.5825]\n",
            "0.24 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5416\n",
            "Hitrate of our first recommendation was: [0.3004, 0.3528, 0.3784, 0.3964, 0.4048, 0.4412]\n",
            "Hitrate of our second recommendation was: [0.3456, 0.4164, 0.4576, 0.49, 0.512, 0.5832]\n",
            "Hitrate of our combined recommendation was: [0.3412, 0.416, 0.4604, 0.4896, 0.5124, 0.5828]\n",
            "0.25 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5361538461538462\n",
            "Hitrate of our first recommendation was: [0.2977, 0.3488, 0.3742, 0.3915, 0.4, 0.4365]\n",
            "Hitrate of our second recommendation was: [0.3442, 0.4146, 0.4554, 0.4877, 0.5092, 0.5831]\n",
            "Hitrate of our combined recommendation was: [0.3396, 0.4146, 0.4581, 0.4869, 0.5092, 0.5804]\n",
            "0.26 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.534074074074074\n",
            "Hitrate of our first recommendation was: [0.2956, 0.3459, 0.3711, 0.3885, 0.397, 0.433]\n",
            "Hitrate of our second recommendation was: [0.3437, 0.4141, 0.4548, 0.487, 0.5081, 0.5815]\n",
            "Hitrate of our combined recommendation was: [0.3389, 0.4133, 0.4567, 0.4859, 0.5081, 0.5785]\n",
            "0.27 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5360714285714285\n",
            "Hitrate of our first recommendation was: [0.2971, 0.3471, 0.3718, 0.3893, 0.3975, 0.4343]\n",
            "Hitrate of our second recommendation was: [0.3446, 0.4154, 0.4554, 0.4864, 0.5079, 0.5821]\n",
            "Hitrate of our combined recommendation was: [0.3396, 0.4139, 0.4568, 0.4854, 0.5071, 0.5793]\n",
            "0.28 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5365517241379311\n",
            "Hitrate of our first recommendation was: [0.2979, 0.349, 0.3741, 0.3917, 0.4003, 0.4366]\n",
            "Hitrate of our second recommendation was: [0.3445, 0.4166, 0.4566, 0.4869, 0.5083, 0.5838]\n",
            "Hitrate of our combined recommendation was: [0.3397, 0.4148, 0.4586, 0.4872, 0.5093, 0.5817]\n",
            "0.29 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5356666666666666\n",
            "Hitrate of our first recommendation was: [0.296, 0.3467, 0.3723, 0.39, 0.399, 0.436]\n",
            "Hitrate of our second recommendation was: [0.3443, 0.416, 0.4563, 0.486, 0.5073, 0.5827]\n",
            "Hitrate of our combined recommendation was: [0.3397, 0.4137, 0.4583, 0.4867, 0.5093, 0.5817]\n",
            "0.3 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5358064516129032\n",
            "Hitrate of our first recommendation was: [0.2952, 0.3458, 0.3713, 0.3887, 0.3977, 0.4358]\n",
            "Hitrate of our second recommendation was: [0.3439, 0.4165, 0.4555, 0.4858, 0.5071, 0.5816]\n",
            "Hitrate of our combined recommendation was: [0.3397, 0.4139, 0.4581, 0.4865, 0.509, 0.581]\n",
            "0.31 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.53625\n",
            "Hitrate of our first recommendation was: [0.2941, 0.3453, 0.3709, 0.3884, 0.3978, 0.4359]\n",
            "Hitrate of our second recommendation was: [0.3428, 0.4156, 0.455, 0.4853, 0.5075, 0.5816]\n",
            "Hitrate of our combined recommendation was: [0.3387, 0.4138, 0.4587, 0.4869, 0.51, 0.5825]\n",
            "0.32 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5390909090909091\n",
            "Hitrate of our first recommendation was: [0.2952, 0.3467, 0.3724, 0.3897, 0.3994, 0.4373]\n",
            "Hitrate of our second recommendation was: [0.3442, 0.417, 0.4555, 0.4867, 0.5094, 0.583]\n",
            "Hitrate of our combined recommendation was: [0.34, 0.4152, 0.4606, 0.4882, 0.5118, 0.5839]\n",
            "0.33 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5391176470588235\n",
            "Hitrate of our first recommendation was: [0.2938, 0.3447, 0.3709, 0.3885, 0.3979, 0.4353]\n",
            "Hitrate of our second recommendation was: [0.3426, 0.4156, 0.4544, 0.4862, 0.5085, 0.5818]\n",
            "Hitrate of our combined recommendation was: [0.3391, 0.4129, 0.4594, 0.4874, 0.5109, 0.5824]\n",
            "0.34 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5414285714285715\n",
            "Hitrate of our first recommendation was: [0.296, 0.3466, 0.372, 0.39, 0.3991, 0.4369]\n",
            "Hitrate of our second recommendation was: [0.3429, 0.418, 0.4566, 0.4877, 0.5097, 0.5826]\n",
            "Hitrate of our combined recommendation was: [0.3397, 0.4146, 0.46, 0.488, 0.5117, 0.5834]\n",
            "0.35 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5372222222222223\n",
            "Hitrate of our first recommendation was: [0.2942, 0.3444, 0.3697, 0.3875, 0.3964, 0.4339]\n",
            "Hitrate of our second recommendation was: [0.3419, 0.4178, 0.4567, 0.4881, 0.51, 0.5819]\n",
            "Hitrate of our combined recommendation was: [0.3392, 0.4136, 0.4592, 0.4872, 0.5108, 0.5828]\n",
            "0.36 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5345945945945946\n",
            "Hitrate of our first recommendation was: [0.2932, 0.3424, 0.3676, 0.3851, 0.3938, 0.4305]\n",
            "Hitrate of our second recommendation was: [0.3424, 0.4176, 0.4557, 0.487, 0.5086, 0.5805]\n",
            "Hitrate of our combined recommendation was: [0.3395, 0.4135, 0.4584, 0.4857, 0.5089, 0.5808]\n",
            "0.37 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.536578947368421\n",
            "Hitrate of our first recommendation was: [0.2958, 0.3466, 0.3713, 0.3887, 0.3974, 0.4337]\n",
            "Hitrate of our second recommendation was: [0.3447, 0.4205, 0.4592, 0.4905, 0.5116, 0.5829]\n",
            "Hitrate of our combined recommendation was: [0.3418, 0.4176, 0.4621, 0.4895, 0.5124, 0.5832]\n",
            "0.38 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5338461538461539\n",
            "Hitrate of our first recommendation was: [0.2946, 0.3446, 0.3687, 0.3859, 0.3949, 0.4313]\n",
            "Hitrate of our second recommendation was: [0.3446, 0.4203, 0.4587, 0.4897, 0.5103, 0.5818]\n",
            "Hitrate of our combined recommendation was: [0.3421, 0.4169, 0.4615, 0.4885, 0.511, 0.5815]\n",
            "0.39 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.53575\n",
            "Hitrate of our first recommendation was: [0.2955, 0.346, 0.3703, 0.3877, 0.3965, 0.433]\n",
            "Hitrate of our second recommendation was: [0.3458, 0.4215, 0.4602, 0.4915, 0.5115, 0.5833]\n",
            "Hitrate of our combined recommendation was: [0.3427, 0.4183, 0.463, 0.4908, 0.5132, 0.5837]\n",
            "0.4 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5358536585365854\n",
            "Hitrate of our first recommendation was: [0.2949, 0.3454, 0.369, 0.3871, 0.3959, 0.4324]\n",
            "Hitrate of our second recommendation was: [0.3451, 0.4202, 0.4593, 0.49, 0.5102, 0.5824]\n",
            "Hitrate of our combined recommendation was: [0.342, 0.4173, 0.462, 0.4895, 0.5122, 0.5829]\n",
            "0.41 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5352380952380953\n",
            "Hitrate of our first recommendation was: [0.295, 0.3452, 0.3688, 0.3869, 0.396, 0.4324]\n",
            "Hitrate of our second recommendation was: [0.3462, 0.4205, 0.46, 0.4905, 0.5112, 0.5836]\n",
            "Hitrate of our combined recommendation was: [0.3431, 0.4186, 0.4629, 0.4902, 0.5131, 0.5836]\n",
            "0.42 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5358139534883721\n",
            "Hitrate of our first recommendation was: [0.2951, 0.3451, 0.3684, 0.3867, 0.396, 0.433]\n",
            "Hitrate of our second recommendation was: [0.3458, 0.4195, 0.4586, 0.4888, 0.5095, 0.5828]\n",
            "Hitrate of our combined recommendation was: [0.3428, 0.4174, 0.4614, 0.4891, 0.5119, 0.5828]\n",
            "0.43 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5356818181818181\n",
            "Hitrate of our first recommendation was: [0.2948, 0.3448, 0.3686, 0.387, 0.3964, 0.4332]\n",
            "Hitrate of our second recommendation was: [0.345, 0.4198, 0.4586, 0.4893, 0.51, 0.5834]\n",
            "Hitrate of our combined recommendation was: [0.3423, 0.4168, 0.4609, 0.4895, 0.5125, 0.5834]\n",
            "0.44 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5344444444444445\n",
            "Hitrate of our first recommendation was: [0.2953, 0.3456, 0.3691, 0.3871, 0.3964, 0.4329]\n",
            "Hitrate of our second recommendation was: [0.3469, 0.422, 0.4604, 0.4918, 0.5122, 0.5849]\n",
            "Hitrate of our combined recommendation was: [0.3442, 0.4196, 0.4633, 0.4916, 0.514, 0.5849]\n",
            "0.45 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.533695652173913\n",
            "Hitrate of our first recommendation was: [0.2946, 0.3443, 0.368, 0.3863, 0.3954, 0.4324]\n",
            "Hitrate of our second recommendation was: [0.3459, 0.4211, 0.4596, 0.4902, 0.5107, 0.5841]\n",
            "Hitrate of our combined recommendation was: [0.3439, 0.4189, 0.4628, 0.4904, 0.5128, 0.5841]\n",
            "0.46 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5321276595744681\n",
            "Hitrate of our first recommendation was: [0.293, 0.3432, 0.3666, 0.3845, 0.3936, 0.4304]\n",
            "Hitrate of our second recommendation was: [0.3451, 0.4206, 0.4589, 0.4898, 0.51, 0.5838]\n",
            "Hitrate of our combined recommendation was: [0.343, 0.4183, 0.4623, 0.4898, 0.5121, 0.5832]\n",
            "0.47 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5325\n",
            "Hitrate of our first recommendation was: [0.2921, 0.3423, 0.366, 0.3842, 0.3937, 0.4306]\n",
            "Hitrate of our second recommendation was: [0.3438, 0.4188, 0.4575, 0.4894, 0.5098, 0.5829]\n",
            "Hitrate of our combined recommendation was: [0.3421, 0.4165, 0.4608, 0.4894, 0.5112, 0.5827]\n",
            "0.48 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5326530612244897\n",
            "Hitrate of our first recommendation was: [0.292, 0.3424, 0.3663, 0.3851, 0.3947, 0.4316]\n",
            "Hitrate of our second recommendation was: [0.3443, 0.4196, 0.4586, 0.4908, 0.512, 0.5849]\n",
            "Hitrate of our combined recommendation was: [0.3424, 0.4173, 0.4618, 0.4908, 0.5131, 0.5853]\n",
            "0.49 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5312\n",
            "Hitrate of our first recommendation was: [0.2912, 0.3414, 0.3648, 0.3832, 0.393, 0.4298]\n",
            "Hitrate of our second recommendation was: [0.3442, 0.4196, 0.4578, 0.4896, 0.5108, 0.5842]\n",
            "Hitrate of our combined recommendation was: [0.3422, 0.4172, 0.4616, 0.4904, 0.5122, 0.584]\n",
            "0.5 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.53\n",
            "Hitrate of our first recommendation was: [0.29, 0.3404, 0.3637, 0.3822, 0.3924, 0.429]\n",
            "Hitrate of our second recommendation was: [0.3433, 0.4184, 0.4571, 0.489, 0.5102, 0.5837]\n",
            "Hitrate of our combined recommendation was: [0.3412, 0.4163, 0.4604, 0.4902, 0.5118, 0.5841]\n",
            "0.51 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5296153846153846\n",
            "Hitrate of our first recommendation was: [0.2896, 0.3394, 0.3627, 0.3808, 0.3912, 0.4279]\n",
            "Hitrate of our second recommendation was: [0.3437, 0.4181, 0.4571, 0.4888, 0.5096, 0.5837]\n",
            "Hitrate of our combined recommendation was: [0.3412, 0.4162, 0.46, 0.4896, 0.5112, 0.5838]\n",
            "0.52 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5298113207547169\n",
            "Hitrate of our first recommendation was: [0.2904, 0.3402, 0.3632, 0.3811, 0.3915, 0.4275]\n",
            "Hitrate of our second recommendation was: [0.3442, 0.4192, 0.4577, 0.4891, 0.5098, 0.5842]\n",
            "Hitrate of our combined recommendation was: [0.3419, 0.417, 0.4604, 0.4898, 0.5119, 0.584]\n",
            "0.53 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5298148148148148\n",
            "Hitrate of our first recommendation was: [0.2894, 0.3389, 0.3615, 0.3793, 0.3898, 0.4269]\n",
            "Hitrate of our second recommendation was: [0.343, 0.4187, 0.4569, 0.4885, 0.5093, 0.5833]\n",
            "Hitrate of our combined recommendation was: [0.3406, 0.4159, 0.4596, 0.4887, 0.5111, 0.5831]\n",
            "0.54 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5289090909090909\n",
            "Hitrate of our first recommendation was: [0.2887, 0.3376, 0.3605, 0.3784, 0.3887, 0.426]\n",
            "Hitrate of our second recommendation was: [0.3425, 0.4178, 0.456, 0.4875, 0.508, 0.5825]\n",
            "Hitrate of our combined recommendation was: [0.34, 0.4153, 0.4587, 0.488, 0.5104, 0.582]\n",
            "0.55 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5282142857142857\n",
            "Hitrate of our first recommendation was: [0.2886, 0.337, 0.3595, 0.3773, 0.3877, 0.4246]\n",
            "Hitrate of our second recommendation was: [0.342, 0.4173, 0.4554, 0.487, 0.5082, 0.582]\n",
            "Hitrate of our combined recommendation was: [0.3398, 0.4146, 0.4584, 0.4873, 0.5098, 0.5814]\n",
            "0.56 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5285964912280702\n",
            "Hitrate of our first recommendation was: [0.2882, 0.3372, 0.3595, 0.3774, 0.3877, 0.4247]\n",
            "Hitrate of our second recommendation was: [0.3412, 0.4168, 0.4547, 0.4865, 0.5081, 0.5821]\n",
            "Hitrate of our combined recommendation was: [0.3391, 0.4142, 0.4581, 0.4874, 0.5095, 0.5809]\n",
            "0.57 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5291379310344828\n",
            "Hitrate of our first recommendation was: [0.2884, 0.3376, 0.3602, 0.3779, 0.3881, 0.4252]\n",
            "Hitrate of our second recommendation was: [0.3424, 0.4174, 0.4555, 0.4874, 0.5088, 0.5824]\n",
            "Hitrate of our combined recommendation was: [0.3402, 0.415, 0.4586, 0.4881, 0.5103, 0.5814]\n",
            "0.58 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5298305084745762\n",
            "Hitrate of our first recommendation was: [0.2881, 0.3373, 0.3602, 0.378, 0.3881, 0.4258]\n",
            "Hitrate of our second recommendation was: [0.3427, 0.4183, 0.4566, 0.488, 0.509, 0.5822]\n",
            "Hitrate of our combined recommendation was: [0.3403, 0.4153, 0.4595, 0.4886, 0.5108, 0.5817]\n",
            "0.59 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5295\n",
            "Hitrate of our first recommendation was: [0.288, 0.337, 0.36, 0.3777, 0.3878, 0.4258]\n",
            "Hitrate of our second recommendation was: [0.3433, 0.4182, 0.4565, 0.4877, 0.5088, 0.5817]\n",
            "Hitrate of our combined recommendation was: [0.341, 0.4155, 0.4595, 0.4885, 0.5108, 0.5812]\n",
            "0.6 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5298360655737705\n",
            "Hitrate of our first recommendation was: [0.2877, 0.3367, 0.36, 0.3775, 0.3877, 0.4257]\n",
            "Hitrate of our second recommendation was: [0.3428, 0.4175, 0.4559, 0.4869, 0.5087, 0.5816]\n",
            "Hitrate of our combined recommendation was: [0.3405, 0.4148, 0.4589, 0.4877, 0.51, 0.581]\n",
            "0.61 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5287096774193548\n",
            "Hitrate of our first recommendation was: [0.2876, 0.336, 0.3594, 0.3766, 0.3866, 0.4247]\n",
            "Hitrate of our second recommendation was: [0.3423, 0.4169, 0.4553, 0.4858, 0.5073, 0.5798]\n",
            "Hitrate of our combined recommendation was: [0.3398, 0.4144, 0.4582, 0.4868, 0.5089, 0.579]\n",
            "0.62 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5287301587301587\n",
            "Hitrate of our first recommendation was: [0.2878, 0.3362, 0.3595, 0.3768, 0.3867, 0.4246]\n",
            "Hitrate of our second recommendation was: [0.3421, 0.417, 0.4554, 0.4862, 0.5075, 0.5798]\n",
            "Hitrate of our combined recommendation was: [0.3395, 0.4143, 0.4586, 0.4871, 0.5092, 0.5794]\n",
            "0.63 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5278125\n",
            "Hitrate of our first recommendation was: [0.2875, 0.3356, 0.3589, 0.3759, 0.3861, 0.4239]\n",
            "Hitrate of our second recommendation was: [0.3406, 0.4155, 0.4544, 0.4848, 0.5062, 0.5787]\n",
            "Hitrate of our combined recommendation was: [0.3384, 0.4136, 0.4573, 0.4863, 0.5083, 0.5783]\n",
            "0.64 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5270769230769231\n",
            "Hitrate of our first recommendation was: [0.2878, 0.3358, 0.3588, 0.376, 0.386, 0.4237]\n",
            "Hitrate of our second recommendation was: [0.3412, 0.4162, 0.4546, 0.4851, 0.5065, 0.5789]\n",
            "Hitrate of our combined recommendation was: [0.3391, 0.4149, 0.458, 0.4868, 0.5088, 0.5788]\n",
            "0.65 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5275757575757576\n",
            "Hitrate of our first recommendation was: [0.2883, 0.3368, 0.3602, 0.3771, 0.3871, 0.4247]\n",
            "Hitrate of our second recommendation was: [0.3418, 0.4164, 0.4548, 0.4853, 0.5067, 0.5795]\n",
            "Hitrate of our combined recommendation was: [0.3395, 0.4153, 0.4585, 0.487, 0.5095, 0.5791]\n",
            "0.66 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5271641791044777\n",
            "Hitrate of our first recommendation was: [0.2884, 0.3364, 0.3599, 0.3767, 0.3869, 0.4239]\n",
            "Hitrate of our second recommendation was: [0.3415, 0.4155, 0.4543, 0.4848, 0.5064, 0.5796]\n",
            "Hitrate of our combined recommendation was: [0.339, 0.4146, 0.4579, 0.4863, 0.509, 0.5784]\n",
            "0.67 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5267647058823529\n",
            "Hitrate of our first recommendation was: [0.2868, 0.3343, 0.3578, 0.375, 0.3854, 0.4226]\n",
            "Hitrate of our second recommendation was: [0.3401, 0.414, 0.4525, 0.4834, 0.5049, 0.5778]\n",
            "Hitrate of our combined recommendation was: [0.3375, 0.4128, 0.456, 0.4846, 0.5072, 0.5768]\n",
            "0.68 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5265217391304348\n",
            "Hitrate of our first recommendation was: [0.2867, 0.3342, 0.358, 0.3754, 0.3858, 0.4229]\n",
            "Hitrate of our second recommendation was: [0.3407, 0.4148, 0.4533, 0.4841, 0.5054, 0.5783]\n",
            "Hitrate of our combined recommendation was: [0.3378, 0.4133, 0.4571, 0.4854, 0.5078, 0.5774]\n",
            "0.69 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5258571428571429\n",
            "Hitrate of our first recommendation was: [0.2847, 0.3324, 0.3564, 0.374, 0.3846, 0.4219]\n",
            "Hitrate of our second recommendation was: [0.3397, 0.4141, 0.4529, 0.4837, 0.5053, 0.5783]\n",
            "Hitrate of our combined recommendation was: [0.3366, 0.4123, 0.4564, 0.4844, 0.5074, 0.5776]\n",
            "0.7 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5253521126760563\n",
            "Hitrate of our first recommendation was: [0.2835, 0.3321, 0.3559, 0.3732, 0.3838, 0.4213]\n",
            "Hitrate of our second recommendation was: [0.3387, 0.4138, 0.4527, 0.4834, 0.5048, 0.578]\n",
            "Hitrate of our combined recommendation was: [0.3356, 0.4121, 0.4566, 0.4844, 0.5072, 0.5773]\n",
            "0.71 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5256944444444445\n",
            "Hitrate of our first recommendation was: [0.2822, 0.3312, 0.356, 0.3732, 0.3837, 0.4214]\n",
            "Hitrate of our second recommendation was: [0.3379, 0.4129, 0.4517, 0.4822, 0.5035, 0.5768]\n",
            "Hitrate of our combined recommendation was: [0.3346, 0.4114, 0.4556, 0.4831, 0.5067, 0.5769]\n",
            "0.72 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5268493150684932\n",
            "Hitrate of our first recommendation was: [0.2832, 0.3318, 0.3564, 0.374, 0.3848, 0.4225]\n",
            "Hitrate of our second recommendation was: [0.3386, 0.413, 0.4518, 0.4819, 0.5036, 0.5768]\n",
            "Hitrate of our combined recommendation was: [0.3352, 0.4119, 0.4559, 0.4833, 0.5071, 0.5773]\n",
            "0.73 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.527972972972973\n",
            "Hitrate of our first recommendation was: [0.2841, 0.3328, 0.3572, 0.3746, 0.3855, 0.4231]\n",
            "Hitrate of our second recommendation was: [0.3392, 0.4134, 0.452, 0.4824, 0.5042, 0.5773]\n",
            "Hitrate of our combined recommendation was: [0.3358, 0.4124, 0.4562, 0.4835, 0.507, 0.5778]\n",
            "0.74 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.528\n",
            "Hitrate of our first recommendation was: [0.2837, 0.3325, 0.3571, 0.3744, 0.3852, 0.4225]\n",
            "Hitrate of our second recommendation was: [0.3387, 0.4128, 0.4524, 0.4828, 0.5043, 0.5773]\n",
            "Hitrate of our combined recommendation was: [0.3352, 0.412, 0.4559, 0.4832, 0.5071, 0.5776]\n",
            "0.75 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5272368421052631\n",
            "Hitrate of our first recommendation was: [0.2829, 0.3318, 0.3562, 0.3733, 0.3841, 0.4213]\n",
            "Hitrate of our second recommendation was: [0.3382, 0.4125, 0.4517, 0.4824, 0.5039, 0.5767]\n",
            "Hitrate of our combined recommendation was: [0.3347, 0.4117, 0.4553, 0.4824, 0.5061, 0.577]\n",
            "0.76 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5272727272727272\n",
            "Hitrate of our first recommendation was: [0.2829, 0.3317, 0.356, 0.373, 0.3838, 0.4213]\n",
            "Hitrate of our second recommendation was: [0.3383, 0.4122, 0.4516, 0.4821, 0.5035, 0.576]\n",
            "Hitrate of our combined recommendation was: [0.3349, 0.4114, 0.4549, 0.4822, 0.5057, 0.5765]\n",
            "0.77 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5266666666666666\n",
            "Hitrate of our first recommendation was: [0.2824, 0.3312, 0.3558, 0.3727, 0.3836, 0.4208]\n",
            "Hitrate of our second recommendation was: [0.3378, 0.4115, 0.4506, 0.4812, 0.5028, 0.5759]\n",
            "Hitrate of our combined recommendation was: [0.3346, 0.411, 0.4547, 0.4819, 0.5053, 0.576]\n",
            "0.78 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5265822784810127\n",
            "Hitrate of our first recommendation was: [0.2823, 0.3306, 0.3552, 0.3719, 0.3829, 0.42]\n",
            "Hitrate of our second recommendation was: [0.3375, 0.4114, 0.4503, 0.4811, 0.5025, 0.5761]\n",
            "Hitrate of our combined recommendation was: [0.3343, 0.4105, 0.4544, 0.4815, 0.5048, 0.5757]\n",
            "0.79 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.526125\n",
            "Hitrate of our first recommendation was: [0.2818, 0.3299, 0.3543, 0.3711, 0.3822, 0.4193]\n",
            "Hitrate of our second recommendation was: [0.3369, 0.4108, 0.4496, 0.4804, 0.5016, 0.5755]\n",
            "Hitrate of our combined recommendation was: [0.3336, 0.4098, 0.4534, 0.4805, 0.504, 0.5747]\n",
            "0.8 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5264197530864198\n",
            "Hitrate of our first recommendation was: [0.2822, 0.3306, 0.3548, 0.3719, 0.3828, 0.4196]\n",
            "Hitrate of our second recommendation was: [0.3374, 0.4112, 0.4501, 0.481, 0.5021, 0.5752]\n",
            "Hitrate of our combined recommendation was: [0.3341, 0.41, 0.454, 0.481, 0.5043, 0.5748]\n",
            "0.81 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5263414634146342\n",
            "Hitrate of our first recommendation was: [0.2813, 0.3298, 0.3538, 0.3707, 0.3816, 0.4191]\n",
            "Hitrate of our second recommendation was: [0.3366, 0.4105, 0.4494, 0.4802, 0.5011, 0.5745]\n",
            "Hitrate of our combined recommendation was: [0.3332, 0.4093, 0.4532, 0.4802, 0.5034, 0.5739]\n",
            "0.82 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5262650602409639\n",
            "Hitrate of our first recommendation was: [0.2811, 0.3295, 0.3535, 0.3705, 0.3813, 0.419]\n",
            "Hitrate of our second recommendation was: [0.336, 0.41, 0.4489, 0.4799, 0.5006, 0.5737]\n",
            "Hitrate of our combined recommendation was: [0.3327, 0.409, 0.4527, 0.4796, 0.5028, 0.5733]\n",
            "0.83 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5264285714285715\n",
            "Hitrate of our first recommendation was: [0.2817, 0.3298, 0.3535, 0.3705, 0.3814, 0.4194]\n",
            "Hitrate of our second recommendation was: [0.3368, 0.4104, 0.4493, 0.48, 0.5006, 0.5737]\n",
            "Hitrate of our combined recommendation was: [0.3336, 0.4096, 0.453, 0.4798, 0.503, 0.5731]\n",
            "0.84 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5254117647058824\n",
            "Hitrate of our first recommendation was: [0.2809, 0.3289, 0.3525, 0.3694, 0.3805, 0.4185]\n",
            "Hitrate of our second recommendation was: [0.3361, 0.41, 0.4492, 0.4798, 0.5005, 0.5738]\n",
            "Hitrate of our combined recommendation was: [0.3332, 0.4091, 0.4527, 0.4794, 0.5026, 0.5729]\n",
            "0.85 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5254651162790698\n",
            "Hitrate of our first recommendation was: [0.281, 0.3294, 0.3529, 0.3697, 0.3806, 0.4186]\n",
            "Hitrate of our second recommendation was: [0.3359, 0.4095, 0.4491, 0.4794, 0.5001, 0.5735]\n",
            "Hitrate of our combined recommendation was: [0.333, 0.4091, 0.4526, 0.4792, 0.5021, 0.5727]\n",
            "0.86 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5260919540229885\n",
            "Hitrate of our first recommendation was: [0.2807, 0.329, 0.3524, 0.3692, 0.3802, 0.418]\n",
            "Hitrate of our second recommendation was: [0.3357, 0.4094, 0.449, 0.4791, 0.4998, 0.5726]\n",
            "Hitrate of our combined recommendation was: [0.333, 0.4089, 0.4522, 0.4789, 0.5017, 0.572]\n",
            "0.87 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5268181818181819\n",
            "Hitrate of our first recommendation was: [0.2811, 0.3297, 0.353, 0.3695, 0.3806, 0.4184]\n",
            "Hitrate of our second recommendation was: [0.3351, 0.4098, 0.4492, 0.4791, 0.4998, 0.5726]\n",
            "Hitrate of our combined recommendation was: [0.3326, 0.4091, 0.4522, 0.4786, 0.5015, 0.5718]\n",
            "0.88 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5261797752808989\n",
            "Hitrate of our first recommendation was: [0.2815, 0.3297, 0.353, 0.3694, 0.3806, 0.4184]\n",
            "Hitrate of our second recommendation was: [0.3358, 0.4102, 0.4494, 0.4792, 0.4998, 0.5725]\n",
            "Hitrate of our combined recommendation was: [0.3333, 0.4096, 0.4524, 0.4787, 0.5012, 0.5717]\n",
            "0.89 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5253333333333333\n",
            "Hitrate of our first recommendation was: [0.281, 0.3289, 0.352, 0.3687, 0.38, 0.4179]\n",
            "Hitrate of our second recommendation was: [0.3354, 0.4094, 0.4487, 0.4782, 0.4988, 0.5712]\n",
            "Hitrate of our combined recommendation was: [0.3329, 0.4089, 0.4514, 0.4774, 0.5002, 0.5707]\n",
            "0.9 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5254945054945055\n",
            "Hitrate of our first recommendation was: [0.2812, 0.3287, 0.3518, 0.3688, 0.3801, 0.418]\n",
            "Hitrate of our second recommendation was: [0.3358, 0.4097, 0.4487, 0.4781, 0.4986, 0.5714]\n",
            "Hitrate of our combined recommendation was: [0.3334, 0.4092, 0.4515, 0.4774, 0.5002, 0.5709]\n",
            "0.91 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5258695652173913\n",
            "Hitrate of our first recommendation was: [0.2822, 0.3295, 0.3526, 0.3696, 0.3809, 0.4186]\n",
            "Hitrate of our second recommendation was: [0.3366, 0.4107, 0.4499, 0.479, 0.4993, 0.572]\n",
            "Hitrate of our combined recommendation was: [0.3343, 0.4098, 0.4526, 0.4783, 0.501, 0.5713]\n",
            "0.92 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.525483870967742\n",
            "Hitrate of our first recommendation was: [0.2818, 0.3294, 0.3527, 0.3696, 0.3809, 0.4184]\n",
            "Hitrate of our second recommendation was: [0.3371, 0.4108, 0.45, 0.4792, 0.5001, 0.5726]\n",
            "Hitrate of our combined recommendation was: [0.3348, 0.4098, 0.4531, 0.4786, 0.5015, 0.5718]\n",
            "0.93 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5256382978723404\n",
            "Hitrate of our first recommendation was: [0.2816, 0.3297, 0.3531, 0.3701, 0.3813, 0.4188]\n",
            "Hitrate of our second recommendation was: [0.3369, 0.4111, 0.4499, 0.4791, 0.5001, 0.5724]\n",
            "Hitrate of our combined recommendation was: [0.3347, 0.41, 0.4534, 0.4791, 0.5018, 0.5717]\n",
            "0.94 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.526\n",
            "Hitrate of our first recommendation was: [0.282, 0.3299, 0.3534, 0.3702, 0.3814, 0.4188]\n",
            "Hitrate of our second recommendation was: [0.3375, 0.4112, 0.4501, 0.4793, 0.5002, 0.5726]\n",
            "Hitrate of our combined recommendation was: [0.3352, 0.4102, 0.4537, 0.4794, 0.5019, 0.5716]\n",
            "0.95 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5252083333333334\n",
            "Hitrate of our first recommendation was: [0.2819, 0.3297, 0.353, 0.3698, 0.381, 0.4182]\n",
            "Hitrate of our second recommendation was: [0.3375, 0.4116, 0.4505, 0.4796, 0.5003, 0.5725]\n",
            "Hitrate of our combined recommendation was: [0.3354, 0.4106, 0.454, 0.4796, 0.5021, 0.5714]\n",
            "0.96 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5251546391752577\n",
            "Hitrate of our first recommendation was: [0.2819, 0.33, 0.3533, 0.37, 0.3812, 0.4181]\n",
            "Hitrate of our second recommendation was: [0.3377, 0.4123, 0.4516, 0.4805, 0.5014, 0.5734]\n",
            "Hitrate of our combined recommendation was: [0.3357, 0.4113, 0.4551, 0.4807, 0.5033, 0.5722]\n",
            "0.97 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5248979591836734\n",
            "Hitrate of our first recommendation was: [0.2816, 0.3298, 0.3532, 0.3698, 0.381, 0.4179]\n",
            "Hitrate of our second recommendation was: [0.3368, 0.4117, 0.4509, 0.4802, 0.5013, 0.5733]\n",
            "Hitrate of our combined recommendation was: [0.3351, 0.4108, 0.4545, 0.4804, 0.5031, 0.572]\n",
            "0.98 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5247474747474747\n",
            "Hitrate of our first recommendation was: [0.2812, 0.3291, 0.3526, 0.3693, 0.3807, 0.4174]\n",
            "Hitrate of our second recommendation was: [0.337, 0.4116, 0.4511, 0.4803, 0.5015, 0.5734]\n",
            "Hitrate of our combined recommendation was: [0.3351, 0.4105, 0.4544, 0.4804, 0.503, 0.5722]\n",
            "0.99 ----------------------------------------------------\n",
            "Classifier accuracy on test set was: 0.5252\n",
            "Hitrate of our first recommendation was: [0.2814, 0.329, 0.3526, 0.3694, 0.3811, 0.4179]\n",
            "Hitrate of our second recommendation was: [0.337, 0.4116, 0.4514, 0.4804, 0.5015, 0.5737]\n",
            "Hitrate of our combined recommendation was: [0.3354, 0.4103, 0.4544, 0.4803, 0.503, 0.5725]\n",
            "1.0 ----------------------------------------------------\n",
            "[[193, 25, 5, 18, 32, 166, 9, 28, 6, 30], [11, 815, 9, 26, 48, 163, 29, 46, 10, 23], [5, 10, 234, 11, 16, 47, 25, 13, 2, 21], [39, 88, 30, 401, 76, 236, 60, 191, 32, 84], [37, 74, 13, 58, 400, 282, 70, 102, 29, 49], [95, 189, 30, 67, 179, 1215, 168, 162, 38, 112], [9, 47, 29, 21, 36, 123, 437, 23, 27, 35], [12, 54, 7, 56, 50, 166, 10, 881, 15, 48], [15, 14, 2, 22, 26, 52, 32, 21, 209, 84], [10, 19, 8, 21, 20, 98, 26, 46, 39, 468]]\n",
            "Classifier accuracy on test set was: 0.5253\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2e87ae286532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classifier accuracy on test set was:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hitrate of our recommender was:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
          ]
        }
      ],
      "source": [
        "# Check accuracy of classifier\n",
        "ks = [2, 4, 6, 8, 10, 20]\n",
        "cm = [[0 for i in range(len(category.keys()))] for j in range(len(category.keys()))]\n",
        "hr_1 = [0]*len(ks)\n",
        "hr_2 = [0]*len(ks)\n",
        "hr_3 = [0]*len(ks)\n",
        "acc = 0\n",
        "count = 0\n",
        "for vid, test_row in test.iterrows():\n",
        "\n",
        "  # Alive check\n",
        "  count += 1\n",
        "  if count % 100 == 0:\n",
        "    print(\"Classifier accuracy on test set was:\", acc/count)\n",
        "    print(\"Hitrate of our first recommendation was:\", [round(i/count, 4) for i in hr_1])\n",
        "    print(\"Hitrate of our second recommendation was:\", [round(i/count, 4) for i in hr_2])\n",
        "    print(\"Hitrate of our combined recommendation was:\", [round(i/count, 4) for i in hr_3])\n",
        "    print(round(count/len(test), 3), \"----------------------------------------------------\")\n",
        "\n",
        "  # Determine evaluation metrics\n",
        "  prediction = classifier.predict(np.array([test_row[\"content\"]]), verbose = 0)[0]\n",
        "  pc = np.argmax(prediction)\n",
        "  tc = np.argmax(test_row[\"label\"])\n",
        "  cm[tc][pc] += 1\n",
        "  acc += 1 if pc == tc else 0\n",
        "  a = encoder([test_row[\"content\"]])[0]\n",
        "\n",
        "\n",
        "  # Compute cosine similarities\n",
        "  def similarity(row):\n",
        "    if category[row[\"category_id\"]] == np.argmax(prediction):\n",
        "      return 1-np.arccos(np.dot(a, row[\"embedding\"]) / (np.linalg.norm(a)*np.linalg.norm(row[\"embedding\"])))/math.pi\n",
        "    else:\n",
        "      return 0\n",
        "  sim_1 = data.apply(lambda row: similarity(row), axis=1)\n",
        "  sim_2 = data.apply(lambda row: (1-np.arccos(np.dot(a, row[\"embedding\"]) / (np.linalg.norm(a)*np.linalg.norm(row[\"embedding\"])))/math.pi), axis=1)\n",
        "\n",
        "  # Assemble recommendations\n",
        "  recommendations_1 = list(sim_1.nlargest(max(ks)).index)\n",
        "  recommendations_2 = list(sim_2.nlargest(max(ks)).index)\n",
        "  recommendations = []\n",
        "  for i in range(max(ks)):\n",
        "    if recommendations_2[i] not in recommendations:\n",
        "      recommendations.append(recommendations_2[i])\n",
        "    if recommendations_1[i] not in recommendations:\n",
        "      recommendations.append(recommendations_1[i])\n",
        "\n",
        "  # Assemble recommendations\n",
        "  for j in range(len(ks)):\n",
        "    k = ks[j]\n",
        "    hr_1[j] += 1 if vid in recommendations_1[:k] else 0\n",
        "    hr_2[j] += 1 if vid in recommendations_2[:k] else 0\n",
        "    hr_3[j] += 1 if vid in recommendations[:k] else 0\n",
        "\n",
        "print(cm)\n",
        "print(\"Classifier accuracy on test set was:\", acc/len(test))\n",
        "print(\"Hitrate of our recommender was:\", hr_3/len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxZMjwsBH62N"
      },
      "source": [
        "For playing around"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti3AdeAHBNIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5e03c1-23d5-4690-b50e-aa1086bf4d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.youtube.com/watch?v=B9SptdjpJBQ\n",
            "https://www.youtube.com/watch?v=5e0LMJRJFaY\n",
            "https://www.youtube.com/watch?v=9yUZTTLpDtk\n",
            "https://www.youtube.com/watch?v=mceaM2_zQd8\n",
            "https://www.youtube.com/watch?v=fLLe5sODKtg\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"\n",
        "Dear students,\n",
        "\n",
        "January 20th the theory exam of the Information Retrieval course will take place. As mentioned in the course description, the exam is closed-book but you are allowed to use a hand-written formularium:\n",
        "\n",
        "    Closed book exam (50% of grade): paper based exam regarding the thory of the course. Students are allowed to use a hand-written formularium.\n",
        "\n",
        "Concretely you are allowed to bring with you: a maximum of 10 a4-sized pages of handwritten notes. There is no restriction on what you can include in these pages; algorithms, formulas, a sort summary of the course, ...\n",
        "\n",
        "Best regards,\n",
        "Toon Calders\n",
        "\"\"\"\n",
        "\n",
        "k = 5\n",
        "\n",
        "# Determine accuracy\n",
        "prediction = classifier.predict(np.array([query]), verbose = 0)[0]\n",
        "temp = {v: prediction[i] for v,i in category.items()}\n",
        "acc += 1 if np.argmax(prediction) == np.argmax(test_row[\"label\"]) else 0\n",
        "\n",
        "# Compute cosine similarities\n",
        "a = encoder([query])[0]\n",
        "\n",
        "# Compute cosine similarities\n",
        "def similarity(row):\n",
        "  if category[row[\"category_id\"]] == np.argmax(prediction):\n",
        "    return 1-np.arccos(np.dot(a, row[\"embedding\"]) / (np.linalg.norm(a)*np.linalg.norm(row[\"embedding\"])))/math.pi\n",
        "  else:\n",
        "    return 0\n",
        "sim_1 = data.apply(lambda row: similarity(row), axis=1)\n",
        "sim_2 = data.apply(lambda row: (1-np.arccos(np.dot(a, row[\"embedding\"]) / (np.linalg.norm(a)*np.linalg.norm(row[\"embedding\"])))/math.pi), axis=1)\n",
        "\n",
        "# Assemble recommendations\n",
        "recommendations_1 = list(sim_1.nlargest(k).index)\n",
        "recommendations_2 = list(sim_2.nlargest(k).index)\n",
        "recommendations = []\n",
        "for i in range(k):\n",
        "  if recommendations_2[i] not in recommendations:\n",
        "    recommendations.append(recommendations_2[i])\n",
        "  if recommendations_1[i] not in recommendations:\n",
        "    recommendations.append(recommendations_1[i])\n",
        "recommendations = recommendations[:k]\n",
        "\n",
        "# Select top 10\n",
        "for vid in recommendations:\n",
        "  print(\"https://www.youtube.com/watch?v={}\".format(vid))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}